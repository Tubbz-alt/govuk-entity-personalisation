{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth\n",
    "from google.cloud import bigquery\n",
    "import pandas_gbq\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pigeon import annotate\n",
    "import json\n",
    "from src.utils.query_search_api import query_search_api\n",
    "import regex\n",
    "from scipy.stats import sem, t, beta\n",
    "from scipy import mean\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows',20)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_big_query_client():\n",
    "    credentials, project_id = google.auth.default()\n",
    "\n",
    "    return bigquery.Client(\n",
    "      credentials=credentials,\n",
    "      project=project_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = create_big_query_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = os.getenv('PROJECT_DIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best bets available here: https://search-admin.publishing.service.gov.uk/\n",
    "best_bets = pd.read_csv(PROJECT_DIR+'/data/processed/best_bets.csv')\n",
    "#This isn't how best best are implement but near enough\n",
    "best_bets_regex = \"(\" + \"|\".join([w.lower() for w in best_bets['query']]) + \")\"\n",
    "\n",
    "best_bets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampe of queries over 2 years\n",
    "\n",
    "query_sample =\"\"\"SELECT\n",
    "  LOWER(hits.page.searchKeyword) as search_term\n",
    "  ,CONCAT(CAST(fullVisitorId AS STRING), CAST(visitStartTime AS STRING)) AS session_id\n",
    "  ,TIMESTAMP_SECONDS(visitStartTime+CAST(hits.time/1000 AS INT64)) as search_timestamp\n",
    "\n",
    "FROM\n",
    "  `govuk-bigquery-analytics.87773428.ga_sessions_*`,\n",
    "  UNNEST(hits) AS hits\n",
    "WHERE\n",
    "  RIGHT(_table_suffix,2) = '01'\n",
    "  AND hits.page.searchKeyword IS NOT NULL\n",
    "  AND _table_suffix BETWEEN FORMAT_DATE('%Y%m%d',DATE_SUB(CURRENT_DATE(), INTERVAL 24 MONTH))\n",
    "    AND FORMAT_DATE('%Y%m%d',DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY))\n",
    "    \"\"\"\n",
    "\n",
    "#search_df = pandas_gbq.read_gbq(query_sample)\n",
    "#search_df.to_csv(PROJECT_DIR+'/data/processed/search_sample_2_years.csv')\n",
    "\n",
    "search_df = pd.read_csv(PROJECT_DIR+'/data/processed/search_sample_2_years.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add some useful cols to search data\n",
    "search_df['date'] = search_df['search_timestamp'].map(lambda x:x[:7])\n",
    "search_df['tokens'] = search_df['search_term'].map(lambda x:x.split())\n",
    "search_df['query_length'] = search_df['tokens'].map(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If you fancy labelling some queries according to their relvance...\n",
    "\n",
    "If you do, you might want to consider filtering/turning off best bets prior to labelling!\n",
    "\n",
    "#Get top n search responses for each of our sampled queries\n",
    "query_sample = search_df.sample(200)\n",
    "query_sample['top_response']=query_sample['search_term'].map(lambda x:query_search_api(x,5))\n",
    "\n",
    "# Futz around with json fields\n",
    "query_sample = query_sample.explode('top_response')\n",
    "query_sample['description'] = sample['top_response'].map(lambda x:x.get('description',np.nan) if not isinstance(\n",
    "                x, float) else [])\n",
    "query_sample['id'] = sample['top_response'].map(lambda x:x.get('_id',np.nan) if not isinstance(\n",
    "                x, float) else [])\n",
    "query_sample['rank'] = sample['top_response'].map(lambda x:x.get('original_rank',np.nan) if not isinstance(\n",
    "                x, float) else [])\n",
    "query_sample= query_sample.reset_index(drop=True)\n",
    "\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "relevancy_judgments = annotate(query_sample.index,\n",
    "                               options=['very relevant','somewhat relevant','not relevant','no good answer'],\n",
    "                               display_fn=lambda idx : display(sample.loc[idx,['search_term','description','id']]))\n",
    "\n",
    "query_sample['relevancy']= relevancy_judgments\n",
    "\n",
    "query_sample['relevancy'] =  query_sample['relevancy'].map(lambda x: literal_eval(x) if not isinstance(x, float) else np.nan)\n",
    "score = {'not relevant':0,'somewhat relevant':1,'very relevant':2,'no good answer':np.nan}\n",
    "query_sample['score'] = query_sample['relevancy'].map(lambda x: score[x[1]])\n",
    "\n",
    "query_sample.to_csv(PROJECT_DIR+'/data/processed/relevancy_judgements.csv')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query results labelled according (to me!) to their relevance\n",
    "relevancy_judgements = pd.read_csv(PROJECT_DIR +'/data/processed/relevancy_judgements.csv')\n",
    "# Let's just remove best bets from the sample (should have removed them before labelling)\n",
    "relevancy_judgements[\"best_bet\"] = relevancy_judgements['search_term'].map(\n",
    "            lambda x: bool(regex.search(best_bets_regex, x, re.IGNORECASE)) if isinstance(x, str) else False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query length over time\n",
    "avg_len=search_df[['date','query_length']].groupby('date').mean().reset_index()\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(avg_len['date'], avg_len['query_length'], color='green')\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Average length\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query length hist (looks exponential-ish)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "plt.style.use('ggplot')\n",
    "plt.hist(search_df['query_length'], color='green',bins=300,density=True)\n",
    "plt.xlabel(\"Query length\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlim(1,15)\n",
    "plt.xticks(np.arange(1, 15, step=1))\n",
    "ax.axvline(x=np.quantile(search_df['query_length'],0.75))\n",
    "fig.set_size_inches(10,5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Lets look at query category (as decided by me). Categories:\n",
    "    # Specific service: query names (or almost names) specific service or form (regardless of whether that service is on gov.uk)\n",
    "    # Specific guidance: query names specific guidance/stats doc on gov uk\n",
    "    # Guidance: query references topic user presumably wants guidance about\n",
    "    # Contact: query explicitly asks for contact, email, phone number, chat etc\n",
    "    # Unknown: mysterious!\n",
    "\n",
    "# If you want to do some labelling:\n",
    "# annotation = annotate(df['search_term'],options=['specific_service','specific_guidance','guidance','contact','unknown'])\n",
    "# query_cats =pd.DataFrame(annotation, columns=['query','category'])\n",
    "# query_cats.to_csv(PROJECT_DIR+'/data/processed/query_categories.csv')\n",
    "query_cats = pd.read_csv(PROJECT_DIR + '/data/processed/query_categories.csv')\n",
    "query_cat=query_cats.groupby('category').size().reset_index(name='cat_count').sort_values('cat_count', ascending=False)\n",
    "query_cat['Percentage']=query_cat['cat_count'] / query_cat['cat_count'].sum() *100\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.bar(query_cat['category'], query_cat['Percentage'], color='green')\n",
    "plt.xlabel(\"Query category\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"%\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision at k (k=5)\n",
    "# https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Precision_at_K\n",
    "\n",
    "no_bestbets = relevancy_judgements[relevancy_judgements['best_bet']==False].dropna()\n",
    "precision = no_bestbets[['query_length','session_id','search_term','score']][no_bestbets['score']>1].groupby(['search_term','session_id','query_length']).count()\n",
    "precision['score'] = precision['score'] / 5\n",
    "precision = precision.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg Precision at 5 \n",
    "\n",
    "avg_score = precision[['query_length','score']].groupby('query_length').agg(['mean','size','sem'])\n",
    "avg_score = avg_score.droplevel(0,1)\n",
    "avg_score['h'] = avg_score['sem'] * t.ppf((1 + 0.95) / 2, avg_score['size'] - 1)\n",
    "avg_score['lower'] = avg_score['mean'] - avg_score['h']\n",
    "avg_score['upper'] = avg_score['mean'] + avg_score['h']\n",
    "avg_score = avg_score.reset_index()\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(10,5))\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "plt.plot(avg_score['query_length'], avg_score['mean'], color='green')\n",
    "ax.fill_between(avg_score['query_length'],avg_score['lower'],avg_score['upper'] , color='b', alpha=.1)\n",
    "plt.xlabel(\"Query length\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Average Precision @5\")\n",
    "fig.set_size_inches(10,5)\n",
    "plt.xlim((1,5))\n",
    "plt.ylim((0,1))\n",
    "plt.xticks(np.arange(1, 6, step=1))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average max scoring doc in top 5 \n",
    "\n",
    "vrel_rate = no_bestbets.groupby('search_term').max('score').reset_index()\n",
    "vrel_rate = vrel_rate[['search_term','query_length','score']].groupby('query_length').agg(['mean','size','sem'])\n",
    "vrel_rate = vrel_rate.droplevel(0,1)\n",
    "vrel_rate['h'] = vrel_rate['sem'] * t.ppf((1 + 0.95) / 2, vrel_rate['size'] - 1)\n",
    "vrel_rate['lower'] = vrel_rate['mean'] - vrel_rate['h']\n",
    "vrel_rate['upper'] = vrel_rate['mean'] + vrel_rate['h']\n",
    "vrel_rate = vrel_rate.reset_index()\n",
    "vrel_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score vs query length\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(10,5))\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "plt.plot(vrel_rate['query_length'], vrel_rate['mean'], color='green')\n",
    "ax.fill_between(vrel_rate['query_length'],vrel_rate['lower'],vrel_rate['upper'] , color='b', alpha=.1)\n",
    "plt.xlabel(\"Query length\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Average max scoring doc\")\n",
    "fig.set_size_inches(10,5)\n",
    "plt.xlim(1,6)\n",
    "plt.ylim(0,3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the rate of returning a very relevant result within top 5 results\n",
    "# It's 45%-65%\n",
    "no_bestbets['very_relevant'] = no_bestbets['score'].map(lambda x: x>1)\n",
    "success_rate = no_bestbets.groupby(['session_id','search_term']).max('score').groupby('very_relevant').size().reset_index(name='count')\n",
    "\n",
    "# Successful very relevant results\n",
    "a = success_rate.iloc[1]['count']\n",
    "# Not very relevant results\n",
    "b = success_rate.iloc[0]['count']\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "x = np.linspace(beta.ppf(0.01,a, b),\n",
    "                beta.ppf(0.99, a, b), 100)\n",
    "\n",
    "\n",
    "lower=beta.ppf(0.025, a, b, loc=0, scale=1)\n",
    "upper=beta.ppf(0.975, a, b, loc=0, scale=1)\n",
    "ax.plot(x, beta.pdf(x,a,b),\n",
    "       'r-', lw=5, alpha=0.6, label='beta pdf')\n",
    "\n",
    "plt.ylabel(\"Density\")\n",
    "plt.xlabel('Very relevant rate')\n",
    "fig.set_size_inches(10,5)\n",
    "ax.axvline(x=lower)\n",
    "ax.axvline(x=upper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
