{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth\n",
    "from google.cloud import bigquery\n",
    "import pandas_gbq\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from py2neo import Graph\n",
    "import os\n",
    "from ast import literal_eval\n",
    "import json\n",
    "from py2neo import Graph\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "import string\n",
    "from scipy.stats import entropy\n",
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "PROJECT_DIR = os.getenv('PROJECT_DIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to KG\n",
    "graph = Graph(host='knowledge-graph.integration.govuk.digital',auth=(os.getenv('NEO_USER'),os.getenv('NEO_PASSWORD')), secure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to BQ\n",
    "# Need to have active environment variable called GOOGLE_APPLICATION_CREDENTIALS pointing to json file with\n",
    "# bigquery credentials\n",
    "def create_big_query_client():\n",
    "    credentials, project_id = google.auth.default()\n",
    "    return bigquery.Client(\n",
    "      credentials=credentials,\n",
    "      project=project_id)\n",
    "\n",
    "client = create_big_query_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets every page view prior to a search in a session. Multiple search queries per session are grouped together in a list\n",
    "# Over a couple days its a few gigs, but the more data the better obvs\n",
    "# e.g see below\n",
    "\"\"\"\n",
    "session_id | viewed_page | search_terms \n",
    "    123    |    /mot     |  mot, mot check\n",
    "    123    | /check-mot  |  mot, mot check \n",
    "\"\"\"\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    country,\n",
    "    region,\n",
    "    metro,\n",
    "    action.session_id,\n",
    "    viewedpages.pageTitle,\n",
    "    viewedpages.pagePath,\n",
    "    first_search_timestamp,\n",
    "    search_terms,\n",
    "    pageview_timestamp,\n",
    "    ROW_NUMBER() OVER (PARTITION BY action.session_id ORDER BY pageview_timestamp DESC) as hit_n\n",
    "    FROM (\n",
    "      SELECT\n",
    "          geoNetwork.country,\n",
    "          geoNetwork.region,\n",
    "          geoNetwork.metro,\n",
    "          CONCAT(CAST(fullVisitorId AS STRING), CAST(visitId AS STRING)) AS session_id,\n",
    "          string_agg(LOWER(hits.page.searchKeyword)) as search_terms,\n",
    "          MIN(TIMESTAMP_SECONDS(visitStartTime+CAST(hits.time/1000 AS INT64))) as first_search_timestamp\n",
    "          FROM\n",
    "          `govuk-bigquery-analytics.87773428.ga_sessions_*`,\n",
    "          UNNEST(hits) AS hits\n",
    "          WHERE\n",
    "          _table_suffix BETWEEN FORMAT_DATE('%Y%m%d',DATE_SUB(CURRENT_DATE(), INTERVAL 2 DAY))\n",
    "              AND FORMAT_DATE('%Y%m%d',DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY))\n",
    "          AND hits.page.searchKeyword IS NOT NULL\n",
    "          GROUP BY session_id, geoNetwork.region,geoNetwork.metro, geoNetwork.country\n",
    "      ) AS action\n",
    "LEFT JOIN (\n",
    "    SELECT\n",
    "        CONCAT(CAST(fullVisitorId AS STRING), CAST(visitId AS STRING)) AS session_id,\n",
    "        hits.page.pageTitle as pageTitle,\n",
    "        hits.page.pagePath as pagePath,\n",
    "        TIMESTAMP_SECONDS(visitStartTime+CAST(hits.time/1000 AS INT64)) as pageview_timestamp,\n",
    "        FROM\n",
    "        `govuk-bigquery-analytics.87773428.ga_sessions_*` \n",
    "        CROSS JOIN UNNEST(hits) AS hits\n",
    "        WHERE _table_suffix BETWEEN FORMAT_DATE('%Y%m%d',DATE_SUB(CURRENT_DATE(), INTERVAL 2 DAY))\n",
    "          AND FORMAT_DATE('%Y%m%d',DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY)) \n",
    "        AND hits.type = \"PAGE\"\n",
    "      ) as viewedpages\n",
    "ON viewedpages.session_id = action.session_id\n",
    "WHERE pageTitle is not null\n",
    "AND pageview_timestamp < first_search_timestamp\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#queries_df = pandas_gbq.read_gbq(query)\n",
    "#queries_df.to_csv(PROCESSED_DATA+'/pagehistoryqueries.csv')\n",
    "queries_df = pd.read_csv(PROJECT_DIR+'/data/processed/pagehistoryqueries.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query + session data\n",
    "queries_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taxons for every bit of content \n",
    "#taxons_df =graph.run(\"MATCH (c:Cid)-[r:IS_TAGGED_TO]->(t:Taxon)RETURN c.name as pagePath,t.name as taxon\").to_data_frame()\n",
    "#taxons_df.to_csv(PROCESSED_DATA+'/taxons.csv')\n",
    "taxons_df =pd.read_csv(PROJECT_DIR+'/data/processed/taxons.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner join taxons to session/query data \n",
    "# We'll end up with multiple taxons per query \n",
    "# ie each query has potentially many pages viewed prior to the query being made AND pages can have multiple taxons\n",
    "\n",
    "query_taxons = queries_df.merge(taxons_df,on='pagePath')\n",
    "# Split queries up (they're stupidily aggregated in the sql)\n",
    "query_taxons['query']=query_taxons['search_terms'].map(lambda x: x.split(','))\n",
    "query_taxons = query_taxons.explode('query')\n",
    "# Average number of taxons visited per query (around 7)\n",
    "np.mean(query_taxons.groupby('session_id').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of bag of words, let's try bag of entities\n",
    "entities = pd.read_csv(PROJECT_DIR+'/data/processed/content_entities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some multiword tokens\n",
    "\n",
    "\n",
    "entities['tokens'] = entities['name'].map(lambda x: tuple(x.split()))\n",
    "tokenizer = MWETokenizer(list(set(entities['tokens'])))\n",
    "tokenizer.add_mwe([('log', 'in'), ('sign', 'in'), ('sign', 'up')])\n",
    "entities['entity'] = entities['tokens'].map(lambda x: tokenizer.tokenize(x))\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_set = set(entities.explode('entity')['entity'])\n",
    "entity_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "# Tokenise query, explode, and regroup by taxon \n",
    "# End up with a list of tokens per taxon\n",
    "query_taxons['tokens'] = query_taxons['query'].map(lambda x: tokenizer.tokenize(x.translate(translator).split()))\n",
    "query_taxons['entities'] = query_taxons['tokens'].map(lambda x: [token for token in x if token in entity_set])\n",
    "query_taxons = query_taxons.explode('entities')\n",
    "query_taxons = query_taxons[['taxon','entities','tokens']]\n",
    "query_taxons = query_taxons.dropna()\n",
    "# Group query tokens by taxon so we have a list of tokens per taxon\n",
    "query_taxons = query_taxons.groupby('taxon').aggregate(lambda x: list(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_df = pd.read_csv(PROJECT_DIR+'/data/processed/mainstreamcontent.csv')\n",
    "title_df = title_df.dropna()\n",
    "title_df['body_text'] = title_df['text'] + (title_df['title'] + ' ') * 5\n",
    "title_df['body_tokens'] = title_df['body_text'].map(lambda x: tokenizer.tokenize(x.lower().translate(translator).split()))\n",
    "title_df['entities']  = title_df['body_tokens'].map(lambda x: [entity for entity in x if entity in entity_set])\n",
    "title_df['entity_count'] = title_df['entities'].map(Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_taxons['entity_count'] = query_taxons['entities'].map(Counter)\n",
    "query_boe = pd.DataFrame.from_records(query_taxons['entity_count'],index=query_taxons['taxon'])\n",
    "query_boe = query_boe.replace(np.nan,0) \n",
    "query_dis = query_boe.divide(query_boe.sum(1),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words for mainstream content\n",
    "service_boe = pd.DataFrame.from_records(title_df['entity_count'],index=title_df['name'],columns=query_boe.columns)\n",
    "service_boe = service_boe.replace(np.nan,0)\n",
    "#add one smoothing\n",
    "service_boe = service_boe +1\n",
    "service_dis = service_boe.divide(service_boe.sum(1),0)\n",
    "service_dis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(query_dis.shape[1]==service_dis.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find min KL divergence content to taxon query terms\n",
    "# https://en.wikipedia.org/wiki/Information_projection\n",
    "# Add one smoothing, which is p, which is q, all make quite a big difference in performance\n",
    "# p is our normalised bag of words from each taxon query\n",
    "# q is every bit of mainstream content normalised bag of words\n",
    "# e.g try 'Blue badges', 'Afghanistan' etc\n",
    "TAXON = \"Stopping or selling your business\"\n",
    "idx = np.argsort(entropy(np.broadcast_to(query_dis.loc[TAXON].to_numpy(),(service_dis.shape[0],service_dis.shape[1])),service_dis,axis=1))[0:15]\n",
    "service_boe.iloc[idx].index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
