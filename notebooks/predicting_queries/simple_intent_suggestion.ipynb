{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth\n",
    "from google.cloud import bigquery\n",
    "import pandas_gbq\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from py2neo import Graph\n",
    "import os\n",
    "from ast import literal_eval\n",
    "import json\n",
    "from py2neo import Graph\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "import string\n",
    "from scipy.stats import entropy\n",
    "from nltk.tokenize import MWETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "PROCESSED_DATA = os.getenv('DIR_DATA_PROCESSED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to KG\n",
    "graph = Graph(host='knowledge-graph.integration.govuk.digital',auth=(os.getenv('NEO_USER'),os.getenv('NEO_PASSWORD')), secure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to BQ\n",
    "# Need to have active environment variable called GOOGLE_APPLICATION_CREDENTIALS pointing to json file with\n",
    "# bigquery credentials\n",
    "def create_big_query_client():\n",
    "    credentials, project_id = google.auth.default()\n",
    "    return bigquery.Client(\n",
    "      credentials=credentials,\n",
    "      project=project_id)\n",
    "\n",
    "client = create_big_query_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets every page view prior to a search in a session. Multiple search queries per session are grouped together in a list\n",
    "# Over a couple days its a few gigs, but the more data the better obvs\n",
    "# e.g see below\n",
    "\"\"\"\n",
    "session_id | viewed_page | search_terms \n",
    "    123    |    /mot     |  mot, mot check\n",
    "    123    | /check-mot  |  mot, mot check \n",
    "\"\"\"\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    country,\n",
    "    region,\n",
    "    metro,\n",
    "    action.session_id,\n",
    "    viewedpages.pageTitle,\n",
    "    viewedpages.pagePath,\n",
    "    first_search_timestamp,\n",
    "    search_terms,\n",
    "    pageview_timestamp,\n",
    "    ROW_NUMBER() OVER (PARTITION BY action.session_id ORDER BY pageview_timestamp DESC) as hit_n\n",
    "    FROM (\n",
    "      SELECT\n",
    "          geoNetwork.country,\n",
    "          geoNetwork.region,\n",
    "          geoNetwork.metro,\n",
    "          CONCAT(CAST(fullVisitorId AS STRING), CAST(visitId AS STRING)) AS session_id,\n",
    "          string_agg(LOWER(hits.page.searchKeyword)) as search_terms,\n",
    "          MIN(TIMESTAMP_SECONDS(visitStartTime+CAST(hits.time/1000 AS INT64))) as first_search_timestamp\n",
    "          FROM\n",
    "          `govuk-bigquery-analytics.87773428.ga_sessions_*`,\n",
    "          UNNEST(hits) AS hits\n",
    "          WHERE\n",
    "          _table_suffix BETWEEN FORMAT_DATE('%Y%m%d',DATE_SUB(CURRENT_DATE(), INTERVAL 2 DAY))\n",
    "              AND FORMAT_DATE('%Y%m%d',DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY))\n",
    "          AND hits.page.searchKeyword IS NOT NULL\n",
    "          GROUP BY session_id, geoNetwork.region,geoNetwork.metro, geoNetwork.country\n",
    "      ) AS action\n",
    "LEFT JOIN (\n",
    "    SELECT\n",
    "        CONCAT(CAST(fullVisitorId AS STRING), CAST(visitId AS STRING)) AS session_id,\n",
    "        hits.page.pageTitle as pageTitle,\n",
    "        hits.page.pagePath as pagePath,\n",
    "        TIMESTAMP_SECONDS(visitStartTime+CAST(hits.time/1000 AS INT64)) as pageview_timestamp,\n",
    "        FROM\n",
    "        `govuk-bigquery-analytics.87773428.ga_sessions_*` \n",
    "        CROSS JOIN UNNEST(hits) AS hits\n",
    "        WHERE _table_suffix BETWEEN FORMAT_DATE('%Y%m%d',DATE_SUB(CURRENT_DATE(), INTERVAL 2 DAY))\n",
    "          AND FORMAT_DATE('%Y%m%d',DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY)) \n",
    "        AND hits.type = \"PAGE\"\n",
    "      ) as viewedpages\n",
    "ON viewedpages.session_id = action.session_id\n",
    "WHERE pageTitle is not null\n",
    "AND pageview_timestamp < first_search_timestamp\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#queries_df = pandas_gbq.read_gbq(query)\n",
    "#queries_df.to_csv(PROCESSED_DATA+'/pagehistoryqueries.csv')\n",
    "queries_df = pd.read_csv(PROCESSED_DATA+'/pagehistoryqueries.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query + session data\n",
    "queries_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taxons for every bit of content \n",
    "#taxons_df =graph.run(\"MATCH (c:Cid)-[r:IS_TAGGED_TO]->(t:Taxon)RETURN c.name as pagePath,t.name as taxon\").to_data_frame()\n",
    "#taxons_df.to_csv(PROCESSED_DATA+'/taxons.csv')\n",
    "taxons_df =pd.read_csv(PROCESSED_DATA+'/taxons.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner join taxons to session/query data \n",
    "# We'll end up with multiple taxons per query \n",
    "# ie each query has potentially many pages viewed prior to the query being made AND pages can have multiple taxons\n",
    "\n",
    "query_taxons = queries_df.merge(taxons_df,on='pagePath')\n",
    "# Split queries up (they're stupidily aggregated in the sql)\n",
    "query_taxons['query']=query_taxons['search_terms'].map(lambda x: x.split(','))\n",
    "query_taxons = query_taxons.explode('query')\n",
    "# Average number of taxons visited per query (around 7)\n",
    "np.mean(query_taxons.groupby('session_id').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "# Some multiword tokens\n",
    "tokenizer = MWETokenizer([('log', 'in'), ('sign', 'in'), ('sign', 'up')])\n",
    "# Tokenise query, explode, and regroup by taxon \n",
    "# End up with a list of tokens per taxon\n",
    "query_taxons['tokens'] = query_taxons['query'].map(lambda x: tokenizer.tokenize(x.translate(translator).split()))\n",
    "query_taxons = query_taxons.explode('tokens')\n",
    "query_taxons = query_taxons[['taxon','tokens']]\n",
    "query_taxons = query_taxons.dropna()\n",
    "# Group query tokens by taxon so we have a list of tokens per taxon\n",
    "query_taxons = query_taxons.groupby('taxon').aggregate(lambda x: list(x)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the maintstream content text,titles from gov uk (assumed targets for users)\n",
    "#title_df =graph.run(\"match (m:Mainstream) return m.name as name, m.title as title, m.text as text\").to_data_frame()\n",
    "#title_df.to_csv(PROCESSED_DATA+'/mainstreamcontent.csv')\n",
    "title_df = pd.read_csv(PROCESSED_DATA+'/mainstreamcontent.csv')\n",
    "title_df = title_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We want to weight titles a bit highly, so err just multiply by 3\n",
    "title_df['body_tokens'] = title_df['text'] + ' ' + (title_df['title'] + ' ' * 3)\n",
    "title_df['body_tokens'] = title_df['body_tokens'].map(lambda x: tokenizer.tokenize(x.translate(translator).split()))\n",
    "title_df['body_tokens'] = title_df['body_tokens'].map(lambda x: [token.lower() for token in x])\n",
    "title_df = title_df.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intersection of words in queries + content\n",
    "service_terms= set(title_df['body_tokens'].explode('body_tokens'))\n",
    "# Lose some stop words\n",
    "intersection = service_terms.difference(set(['i','of','the','a','and','to','you','if','that','then','in','on']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_df['intersect_tokens'] = title_df['body_tokens'].map(lambda x: [token for token in x if token in intersection])\n",
    "title_df['body_token_count'] = title_df['intersect_tokens'].map(lambda x: Counter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words for mainstream content\n",
    "service_bow = pd.DataFrame.from_records(title_df['body_token_count'],index=title_df['name'])\n",
    "service_bow = service_bow.replace(np.nan,0)\n",
    "#add one smoothing\n",
    "service_bow = service_bow +1\n",
    "service_dis = service_bow.divide(service_bow.sum(1),0)\n",
    "service_dis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_taxons = query_taxons[['taxon','tokens']]\n",
    "query_taxons['valid_tokens'] = query_taxons['tokens'].map(lambda x: [token for token in x if token in intersection])\n",
    "query_taxons['valid_token_count'] = query_taxons['valid_tokens'].map(lambda x:Counter(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words for taxon queries\n",
    "query_bow = pd.DataFrame.from_records(query_taxons['valid_token_count'],index=query_taxons['taxon'],columns=service_dis.columns)\n",
    "# Don't smooth unless you have lots of data\n",
    "query_bow = query_bow.replace(np.nan,0)\n",
    "\n",
    "query_dis = query_bow.divide(query_bow.sum(1),0)\n",
    "query_dis.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(query_dis.shape[1]==service_dis.shape[1]==len(intersection))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Find min KL divergence content to taxon query terms\n",
    "# https://en.wikipedia.org/wiki/Information_projection\n",
    "# Add one smoothing, which is p, which is q, all make quite a big difference in performance\n",
    "# p is our normalised bag of words from each taxon query\n",
    "# q is every bit of mainstream content normalised bag of words\n",
    "# e.g try 'Blue badges', 'Afghanistan' etc\n",
    "TAXON = \"Stopping or selling your business\"\n",
    "idx = np.argsort(entropy(np.broadcast_to(query_dis.loc[TAXON].to_numpy(),(service_dis.shape[0],service_dis.shape[1])),service_dis,axis=1))[0:15]\n",
    "service_bow.iloc[idx].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the same with transition checker data\n",
    "# Same query as above but only looking for transition checker pages completed prior to searching\n",
    "acc_query=\"\"\"SELECT\n",
    "  action.session_id,\n",
    "  viewedpages.pageTitle,\n",
    "  viewedpages.pagePath,\n",
    "  first_search_timestamp,\n",
    "  search_terms,\n",
    "  pageview_timestamp,\n",
    "  ROW_NUMBER() OVER (PARTITION BY action.session_id ORDER BY pageview_timestamp DESC) as hit_n\n",
    "  FROM (\n",
    "   SELECT\n",
    "   geoNetwork.country,\n",
    "   geoNetwork.region,\n",
    "   geoNetwork.metro,\n",
    "  CONCAT(CAST(fullVisitorId AS STRING), CAST(visitId AS STRING)) AS session_id,\n",
    "  string_agg(LOWER(hits.page.searchKeyword)) as search_terms,\n",
    "  MIN(TIMESTAMP_SECONDS(visitStartTime+CAST(hits.time/1000 AS INT64))) as first_search_timestamp\n",
    "\n",
    "FROM\n",
    "  `govuk-bigquery-analytics.87773428.ga_sessions_*`,\n",
    "  UNNEST(hits) AS hits\n",
    "WHERE\n",
    "  _table_suffix BETWEEN FORMAT_DATE('%Y%m%d',DATE_SUB(CURRENT_DATE(), INTERVAL 2 DAY))\n",
    "                AND FORMAT_DATE('%Y%m%d',DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY))\n",
    "  AND hits.page.searchKeyword IS NOT NULL\n",
    "   group by session_id, geoNetwork.region,geoNetwork.metro, geoNetwork.country\n",
    "  ) AS action\n",
    "  LEFT JOIN \n",
    "  (SELECT \n",
    "  CONCAT(CAST(fullVisitorId AS STRING), CAST(visitId AS STRING)) AS session_id,\n",
    "  hits.page.pageTitle as pageTitle,\n",
    "  hits.page.pagePath as pagePath,\n",
    "  ROW_NUMBER() OVER (PARTITION BY CONCAT(CAST(fullVisitorId AS STRING), CAST(visitId AS STRING)) ORDER BY hits.time DESC) as last_question,\n",
    "  TIMESTAMP_SECONDS(visitStartTime+CAST(hits.time/1000 AS INT64)) as pageview_timestamp,\n",
    "  FROM\n",
    "  `govuk-bigquery-analytics.87773428.ga_sessions_*` \n",
    "  CROSS JOIN UNNEST(hits) AS hits\n",
    "  WHERE \n",
    "      _table_suffix BETWEEN FORMAT_DATE('%Y%m%d',DATE_SUB(CURRENT_DATE(), INTERVAL 3 DAY))\n",
    "      AND FORMAT_DATE('%Y%m%d',DATE_SUB(CURRENT_DATE(), INTERVAL 1 DAY)) \n",
    "      AND hits.type = \"PAGE\"\n",
    "      AND hits.page.pagePath LIKE '/transition-check/questions?%'\n",
    "  ) as viewedpages\n",
    "  ON viewedpages.session_id = action.session_id\n",
    "  where pageTitle is not null\n",
    "  AND last_question=1\n",
    "  AND pageview_timestamp < first_search_timestamp\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#acc_df = pandas_gbq.read_gbq(acc_query)\n",
    "#acc_df.to_csv(PROCESSED_DATA+'/accounts.csv')\n",
    "#Clean data stored in urls (annoying)\n",
    "#acc_df['responses']= acc_df['pagePath'].map(lambda x:x.split('c[]'))\n",
    "#acc_df = acc_df.explode('responses')\n",
    "#acc_df['clean'] = acc_df['responses'].map(lambda x: x.split('&')[0])\n",
    "#acc_df = acc_df[acc_df['clean']!='/transition-check/questions?page=1']\n",
    "#acc_df = acc_df[acc_df['clean']!='/transition-check/questions?']\n",
    "#acc_df['clean'] = acc_df['clean'].map(lambda x: x.replace('=',''))\n",
    "\n",
    "acc_df = pd.read_csv(PROCESSED_DATA+'/accounts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check whats worth conditioning on\n",
    "acc_df.groupby('clean').size().reset_index(name='count').sort_values('count',ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise search terms\n",
    "acc_df['search_terms'] = acc_df['search_terms'].map(lambda x: x.split(','))\n",
    "acc_df = acc_df.explode('search_terms')\n",
    "acc_df['tokens'] = acc_df['search_terms'].map(lambda x: tokenizer.tokenize(x.translate(translator).split()))\n",
    "acc_df['valid_tokens'] = acc_df['tokens'].map(lambda x: [token for token in x if token in service_terms])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df = acc_df.explode('valid_tokens')\n",
    "acc_df = acc_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try conditioning search terms on nationality\n",
    "nationality = acc_df[acc_df['clean'].isin(['nationality-eu','nationality-uk','nationality-row'])]\n",
    "nationality = nationality[['clean','valid_tokens']].groupby('clean').aggregate(lambda x: list(x))\n",
    "nationality['valid_token_count'] = nationality['valid_tokens'].map(lambda x:Counter(x))\n",
    "nationality = nationality.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words for nationality search terms\n",
    "nat_bow = pd.DataFrame.from_records(nationality['valid_token_count'],columns=service_dis.columns,index=nationality['clean'])                                  \n",
    "nat_bow = nat_bow.replace(np.nan,0) + 1\n",
    "nat_bow = nat_bow.divide(nat_bow.sum(1),0)\n",
    "nat_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(entropy(np.broadcast_to(nat_bow.loc['nationality-uk'].to_numpy(),(service_dis.shape[0],service_dis.shape[1])),service_dis,axis=1))[0:20]\n",
    "service_bow.iloc[idx].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(entropy(np.broadcast_to(nat_bow.loc['nationality-eu'].to_numpy(),(service_dis.shape[0],service_dis.shape[1])),service_dis,axis=1))[0:10]\n",
    "service_bow.iloc[idx].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argsort(entropy(np.broadcast_to(nat_bow.loc['nationality-row'].to_numpy(),(service_dis.shape[0],service_dis.shape[1])),service_dis,axis=1))[0:10]\n",
    "service_bow.iloc[idx].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxons_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look for clusters within a taxon-query distribution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "language": "python",
   "name": ""
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
